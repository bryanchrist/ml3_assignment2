To execute the default application inside the container, run:
singularity run --nv $CONTAINERDIR/pytorch-2.0.1.sif

This container is based on NGC 23.08
https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel-23-08.html#rel-23-08
WARNING: Ignoring invalid distribution -riton-nightly (/sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages)
ERROR: Could not find a version that satisfies the requirement PIL (from versions: none)
ERROR: No matching distribution found for PIL
WARNING: Ignoring invalid distribution -riton-nightly (/sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages)
WARNING: Ignoring invalid distribution -riton-nightly (/sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages)
Using cache found in /home/brc4cb/.cache/torch/hub/pytorch_vision_v0.10.0
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt101_32X8D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt101_32X8D_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
  0%|          | 0/75 [00:00<?, ?it/s]  1%|▏         | 1/75 [02:05<2:34:49, 125.53s/it]  3%|▎         | 2/75 [03:59<2:24:19, 118.62s/it]  4%|▍         | 3/75 [05:52<2:19:33, 116.30s/it]  5%|▌         | 4/75 [07:46<2:16:22, 115.24s/it]  7%|▋         | 5/75 [09:40<2:13:50, 114.72s/it]  8%|▊         | 6/75 [11:33<2:11:22, 114.24s/it]  9%|▉         | 7/75 [13:26<2:09:02, 113.86s/it] 11%|█         | 8/75 [15:19<2:06:54, 113.64s/it] 12%|█▏        | 9/75 [17:12<2:04:43, 113.38s/it] 13%|█▎        | 10/75 [19:06<2:02:53, 113.43s/it] 15%|█▍        | 11/75 [20:59<2:01:06, 113.54s/it] 16%|█▌        | 12/75 [22:53<1:59:15, 113.57s/it] 17%|█▋        | 13/75 [24:47<1:57:20, 113.56s/it] 19%|█▊        | 14/75 [26:40<1:55:26, 113.55s/it] 20%|██        | 15/75 [28:34<1:53:31, 113.53s/it] 21%|██▏       | 16/75 [30:27<1:51:36, 113.50s/it] 23%|██▎       | 17/75 [32:21<1:49:45, 113.54s/it] 24%|██▍       | 18/75 [34:14<1:47:52, 113.56s/it] 25%|██▌       | 19/75 [36:08<1:45:58, 113.55s/it] 27%|██▋       | 20/75 [38:01<1:44:00, 113.47s/it] 28%|██▊       | 21/75 [39:53<1:41:35, 112.88s/it] 29%|██▉       | 22/75 [41:44<1:39:18, 112.42s/it] 31%|███       | 23/75 [43:35<1:36:58, 111.89s/it] 32%|███▏      | 24/75 [45:27<1:35:21, 112.18s/it] 33%|███▎      | 25/75 [47:19<1:33:15, 111.92s/it] 35%|███▍      | 26/75 [49:10<1:31:07, 111.59s/it] 36%|███▌      | 27/75 [51:01<1:29:13, 111.53s/it] 37%|███▋      | 28/75 [52:54<1:27:49, 112.11s/it] 39%|███▊      | 29/75 [54:46<1:25:55, 112.08s/it] 40%|████      | 30/75 [56:40<1:24:16, 112.38s/it] 41%|████▏     | 31/75 [58:31<1:22:12, 112.11s/it] 43%|████▎     | 32/75 [1:00:22<1:20:11, 111.90s/it] 44%|████▍     | 33/75 [1:02:14<1:18:17, 111.84s/it] 45%|████▌     | 34/75 [1:04:05<1:16:18, 111.67s/it] 47%|████▋     | 35/75 [1:05:57<1:14:23, 111.58s/it] 48%|████▊     | 36/75 [1:07:48<1:12:30, 111.54s/it] 49%|████▉     | 37/75 [1:09:40<1:10:40, 111.58s/it] 51%|█████     | 38/75 [1:11:32<1:08:49, 111.61s/it] 52%|█████▏    | 39/75 [1:13:23<1:06:55, 111.55s/it] 53%|█████▎    | 40/75 [1:15:15<1:05:06, 111.61s/it] 55%|█████▍    | 41/75 [1:17:06<1:03:16, 111.66s/it] 56%|█████▌    | 42/75 [1:18:58<1:01:20, 111.54s/it] 57%|█████▋    | 43/75 [1:20:49<59:30, 111.56s/it]   59%|█████▊    | 44/75 [1:22:41<57:38, 111.57s/it] 60%|██████    | 45/75 [1:24:33<55:48, 111.62s/it] 61%|██████▏   | 46/75 [1:26:24<53:58, 111.66s/it] 63%|██████▎   | 47/75 [1:28:16<52:04, 111.59s/it] 64%|██████▍   | 48/75 [1:30:07<50:11, 111.55s/it] 65%|██████▌   | 49/75 [1:31:59<48:21, 111.58s/it] 67%|██████▋   | 50/75 [1:33:50<46:28, 111.55s/it] 68%|██████▊   | 51/75 [1:35:42<44:35, 111.50s/it] 69%|██████▉   | 52/75 [1:37:34<42:46, 111.58s/it] 71%|███████   | 53/75 [1:39:25<40:55, 111.61s/it] 72%|███████▏  | 54/75 [1:41:17<39:05, 111.69s/it] 73%|███████▎  | 55/75 [1:43:09<37:13, 111.66s/it] 75%|███████▍  | 56/75 [1:45:01<35:22, 111.72s/it] 76%|███████▌  | 57/75 [1:46:52<33:31, 111.74s/it] 77%|███████▋  | 58/75 [1:48:44<31:39, 111.71s/it] 79%|███████▊  | 59/75 [1:50:36<29:46, 111.65s/it] 80%|████████  | 60/75 [1:52:27<27:54, 111.64s/it] 81%|████████▏ | 61/75 [1:54:21<26:11, 112.22s/it] 83%|████████▎ | 62/75 [1:56:13<24:17, 112.13s/it] 84%|████████▍ | 63/75 [1:58:06<22:28, 112.36s/it] 85%|████████▌ | 64/75 [1:59:57<20:33, 112.09s/it] 87%|████████▋ | 65/75 [2:01:48<18:38, 111.83s/it] 88%|████████▊ | 66/75 [2:03:40<16:45, 111.69s/it] 89%|████████▉ | 67/75 [2:05:32<14:54, 111.79s/it] 91%|█████████ | 68/75 [2:07:23<13:02, 111.77s/it] 92%|█████████▏| 69/75 [2:09:15<11:10, 111.76s/it] 93%|█████████▎| 70/75 [2:11:07<09:18, 111.71s/it] 95%|█████████▍| 71/75 [2:12:59<07:27, 111.76s/it] 96%|█████████▌| 72/75 [2:14:50<05:35, 111.80s/it] 97%|█████████▋| 73/75 [2:16:42<03:43, 111.86s/it] 99%|█████████▊| 74/75 [2:18:35<01:51, 111.93s/it]100%|██████████| 75/75 [2:20:26<00:00, 111.77s/it]100%|██████████| 75/75 [2:20:26<00:00, 112.35s/it]
Using cache found in /home/brc4cb/.cache/torch/hub/pytorch_vision_v0.10.0
Traceback (most recent call last):
  File "/sfs/qumulo/qhome/brc4cb/ml3_assignment2/resnext_bigger.py", line 371, in <module>
    train_model(model_ft, optimizer_ft, exp_lr_scheduler, train_loader, valid_loader, criterion, num_epochs = 75)
  File "/sfs/qumulo/qhome/brc4cb/ml3_assignment2/resnext_bigger.py", line 281, in train_model
    preds = model(data_inputs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 183, in forward
    return self.module(*inputs[0], **module_kwargs[0])
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torchvision/models/resnet.py", line 274, in _forward_impl
    x = self.layer2(x)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torchvision/models/resnet.py", line 146, in forward
    out = self.conv1(x)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacty of 39.39 GiB of which 184.38 MiB is free. Including non-PyTorch memory, this process has 39.17 GiB memory in use. Of the allocated memory 38.16 GiB is allocated by PyTorch, and 515.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
