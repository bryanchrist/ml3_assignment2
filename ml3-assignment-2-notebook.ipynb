{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Dependencies","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport numpy as np\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, random_split, DataLoader\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom torchvision.utils import make_grid\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom PIL import Image\nfrom collections import OrderedDict\nfrom tqdm import tqdm\nimport random\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom torch import optim\nimport time\nfrom torch.autograd import Variable\n%matplotlib inline","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 1\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"data_dir = \"mliii-assignment2\" # Path to data directory\nlabels = pd.read_csv(os.path.join(data_dir, 'labels.csv'))\nassert(len(os.listdir(os.path.join(data_dir, 'train'))) == len(labels))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder()\nlabels.breed = le.fit_transform(labels.breed)\nlabels.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = labels.id\ny = labels.breed\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,test_size=0.4, random_state=SEED, stratify=y)\nX_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size=0.5, random_state=SEED, stratify=y_valid)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.transforms import Resize, ToTensor, Normalize\n\nmeans = np.load('means.npy')\nstds = np.load('stds.npy')\nclass Dataset_Interpreter(Dataset):\n    def __init__(self, data_path, file_names, labels=None):\n        self.data_path = data_path\n        self.file_names = file_names\n        self.labels = labels\n        self.transforms = transforms\n        self.resize = Resize((224,224))\n        self.normalize = Normalize(means, stds)\n        self.trans = transforms.Compose([\n            transforms.TenCrop((224,224)),\n            transforms.ColorJitter(),   \n            transforms.ToTensor()\n        ])\n        \n    def __len__(self):\n        return (len(self.file_names))\n    \n    def __getitem__(self, idx):\n        img_name = f'{self.file_names.iloc[idx]}.jpg'\n        full_address = os.path.join(self.data_path, img_name)\n        image = Image.open(full_address)\n        label = self.labels.iloc[idx]\n        image = self.resize(image)\n        image = self.trans(image)\n        image = self.normalize(image)\n#         if self.transforms is not None:\n#             image = self.transforms(image)\n        \n        return image, label\n\nclass Dataset_Interpreter_valid_test(Dataset):\n    def __init__(self, data_path, file_names, labels=None):\n        self.data_path = data_path\n        self.file_names = file_names\n        self.labels = labels\n        self.transforms = transforms\n        self.resize = Resize((224,224))\n        self.normalize = Normalize(means, stds)\n        self.trans = transforms.Compose([   \n            transforms.ToTensor()\n        ])\n        \n    def __len__(self):\n        return (len(self.file_names))\n    \n    def __getitem__(self, idx):\n        img_name = f'{self.file_names.iloc[idx]}.jpg'\n        full_address = os.path.join(self.data_path, img_name)\n        image = Image.open(full_address)\n        label = self.labels.iloc[idx]\n        image = self.resize(image)\n        image = self.trans(image)\n        image = self.normalize(image)\n#         if self.transforms is not None:\n#             image = self.transforms(image)\n        \n        return image, label","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_images(images):\n\n    n_images = len(images)\n\n    rows = int(np.sqrt(n_images))\n    cols = int(np.sqrt(n_images))\n\n    fig = plt.figure(figsize=(20,10))\n    for i in range(rows*cols):\n        ax = fig.add_subplot(rows, cols, i+1)\n        ax.set_title(f'{le.inverse_transform([images[i][1]])}')\n        ax.imshow(np.array(images[i][0]))\n        ax.axis('off')\nN_IMAGES = 9\n\ntrain_data = Dataset_Interpreter(data_path=data_dir+'/train/', file_names=X_train, labels=y_train)\nvalid_data = Dataset_Interpreter_valid_test(data_path=data_dir+'/valid/', file_names=X_valid, labels=y_valid)\ntest_data = Dataset_Interpreter_valid_test(data_path=data_dir+'/test/', file_names=X_test, labels=y_test)\nimages = [(image, label) for image, label in [train_data[i] for i in range(N_IMAGES)]] \n#plot_images(images)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"channel_1_mean = np.mean([train_data[x][0][:,:,0] for x in range(0, len(train_data))])","metadata":{}},{"cell_type":"markdown","source":"channel1 = [train_data[x][0][:,:,0] for x in range(0, len(train_data))]","metadata":{}},{"cell_type":"markdown","source":"channel2 = [train_data[x][0][:,:,1] for x in range(0, len(train_data))]\nchannel3 = [train_data[x][0][:,:,2] for x in range(0, len(train_data))]","metadata":{}},{"cell_type":"markdown","source":"channel_2_mean = np.mean(channel2)\nchannel_3_mean = np.mean(channel3)\nchannel_1_std = np.std(channel1)\nchannel_2_std = np.std(channel2)\nchannel_3_std = np.std(channel3)","metadata":{}},{"cell_type":"markdown","source":"means = np.array([channel_1_mean, channel_2_mean, channel_3_mean])\nstds = np.array([channel_1_std, channel_2_std, channel_3_std])\nnp.save('stds', stds)\nnp.save('means', means)","metadata":{}},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# # Data augmentation and normalization for training\n# # Just normalization for validation\n# data_transforms = {\n#     'train': transforms.Compose([\n#         transforms.RandomResizedCrop(224),\n#         transforms.RandomHorizontalFlip(),\n#         transforms.ToTensor(),\n#         transforms.Normalize(means, stds)\n#     ]),\n#     'val': transforms.Compose([\n#         transforms.Resize(256),\n#         transforms.CenterCrop(224),\n#         transforms.ToTensor(),\n#         transforms.Normalize(means, stds)\n#     ]),\n# }\n\n# image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n#                                           data_transforms[x])\n#                   for x in ['train', 'val']}\n# dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n#                                              shuffle=True, num_workers=4)\n#               for x in ['train', 'val']}\n# dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n# class_names = image_datasets['train'].classes\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Model","metadata":{}},{"cell_type":"code","source":"# Create model here \nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torch.backends.cudnn as cudnn\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nfrom PIL import Image\nfrom tempfile import TemporaryDirectory\n\nmodel_ft = models.resnet50(weights='IMAGENET1K_V2')\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 120)\n\nmodel_ft = model_ft.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer_ft = optim.Adam(model_ft.parameters(), [.9, .99], lr=0.0001)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\n# Pytorch train and test sets\n#train = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\nmin_valid_loss = np.inf\n\n# data loader\ntrain_loader = DataLoader(train_data, batch_size = batch_size, shuffle = False)\nvalid_loader= DataLoader(valid_data, batch_size = batch_size, shuffle = False)\ntest_loader = DataLoader(test_data, batch_size = batch_size, shuffle = False)\n\ndef train_model(model, optimizer, train_loader, valid_loader, loss_module, num_epochs=100):\n    \n    train_loss = 0.0\n    # Training loop\n    for epoch in tqdm(range(num_epochs)):\n        # Set model to train mode\n        model.train()\n        for data_inputs, data_labels in train_loader:\n\n            ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n            data_inputs = data_inputs.to(device)\n            data_labels = np.array(data_labels)\n            data_labels = torch.from_numpy(data_labels)\n            data_labels = data_labels.to(device)\n\n            ## Step 2: Run the model on the input data\n            preds = model(data_inputs)\n            preds = preds.squeeze(dim=1) # Output is [Batch size, 1], but we want [Batch size]\n\n            ## Step 3: Calculate the loss\n            loss = loss_module(preds, data_labels)\n            ## Step 4: Perform backpropagation\n            # Before calculating the gradients, we need to ensure that they are all zero.\n            # The gradients would not be overwritten, but actually added to the existing ones.\n            optimizer.zero_grad()\n            # Perform backpropagation\n            loss.backward()\n\n            ## Step 5: Update the parameters\n            optimizer.step()\n            \n            ## Step 6: Add to loss\n            train_loss += loss.item()\n        \n        #Validation Loop\n        valid_loss = 0.0\n        model.eval()     \n        for data_inputs, data_labels in valid_loader:\n            data_inputs = data_inputs.to(device)\n            data_labels = np.array(data_labels)\n            data_labels = torch.from_numpy(data_labels)\n            data_labels = data_labels.to(device)\n\n            ## Step 2: Run the model on the input data\n            preds = model(data_inputs)\n            preds = preds.squeeze(dim=1) # Output is [Batch size, 1], but we want [Batch size]\n\n            ## Step 3: Calculate the loss\n            loss = loss_module(preds, data_labels)\n            valid_loss = loss.item() * data.size(0)\n\n        print(f'Epoch {epoch+1} \\t\\t Training Loss: {train_loss / len(train_loader)} \\t\\t Validation Loss: {valid_loss / len(valid_loader)}')\n        if min_valid_loss > valid_loss:\n            print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n            min_valid_loss = valid_loss\n            # Saving State Dict\n            torch.save(model.state_dict(), 'saved_model.pth')\ntrain_model(model_ft, optimizer_ft, train_loader, valid_loader, criterion)\n#plt.plot(loss_values)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef eval_model(model, data_loader):\n    model.eval() # Set model to eval mode\n    true_preds, num_preds = 0., 0.\n\n    with torch.no_grad(): # Deactivate gradients for the following code\n        for data_inputs, data_labels in data_loader:\n\n            # Determine prediction of model on dev set\n            data_inputs = data_inputs.to(device)\n            data_labels = data_labels.to(device)\n            preds = model(data_inputs)\n            preds = F.softmax(preds, dim=1)\n            max_prob_labels = torch.argmax(preds, dim=1)\n            \n            # Keep records of predictions for the accuracy metric (true_preds=TP+TN, num_preds=TP+TN+FP+FN)\n            true_preds += (max_prob_labels == data_labels).sum().item()\n            num_preds += data_labels.shape[0]\n\n    acc = true_preds / num_preds\n    print(f\"Accuracy of the model: {100.0*acc:4.2f}%\")\n\n# Assuming you have already defined 'model' and 'train_loader'\neval_model(model_ft, train_loader)\neval_model(model_ft, valid_loader)\neval_model(model_ft, test_loader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    # Create a temporary directory to save training checkpoints\n    with TemporaryDirectory() as tempdir:\n        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n\n        torch.save(model.state_dict(), best_model_params_path)\n        best_acc = 0.0\n\n        for epoch in range(num_epochs):\n            print(f'Epoch {epoch}/{num_epochs - 1}')\n            print('-' * 10)\n\n            # Each epoch has a training and validation phase\n            for phase in ['train', 'val']:\n                if phase == 'train':\n                    model.train()  # Set model to training mode\n                else:\n                    model.eval()   # Set model to evaluate mode\n\n                running_loss = 0.0\n                running_corrects = 0\n\n                # Iterate over data.\n                for inputs, labels in dataloaders[phase]:\n                    inputs = inputs.to(device)\n                    labels = labels.to(device)\n\n                    # zero the parameter gradients\n                    optimizer.zero_grad()\n\n                    # forward\n                    # track history if only in train\n                    with torch.set_grad_enabled(phase == 'train'):\n                        outputs = model(inputs)\n                        _, preds = torch.max(outputs, 1)\n                        loss = criterion(outputs, labels)\n\n                        # backward + optimize only if in training phase\n                        if phase == 'train':\n                            loss.backward()\n                            optimizer.step()\n\n                    # statistics\n                    running_loss += loss.item() * inputs.size(0)\n                    running_corrects += torch.sum(preds == labels.data)\n                if phase == 'train':\n                    scheduler.step()\n\n                epoch_loss = running_loss / dataset_sizes[phase]\n                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n                # deep copy the model\n                if phase == 'val' and epoch_acc > best_acc:\n                    best_acc = epoch_acc\n                    torch.save(model.state_dict(), best_model_params_path)\n\n            print()\n\n        time_elapsed = time.time() - since\n        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n        print(f'Best val Acc: {best_acc:4f}')\n\n        # load best model weights\n        model.load_state_dict(torch.load(best_model_params_path))\n    return model","metadata":{}},{"cell_type":"markdown","source":"model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n                       num_epochs=25)","metadata":{}},{"cell_type":"markdown","source":"# Submission CSV","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef generate_submission(predictions, sample_submission_path, output_path):\n    \"\"\"\n    Generate a Kaggle submission file based on the given predictions.\n\n    Parameters:\n    - predictions: A dictionary with image ids as keys and a list of 120 probabilities as values.\n    - sample_submission_path: Path to the provided sample submission file.\n    - output_path: Path to save the generated submission file.\n    \"\"\"\n    # Load the sample submission\n    sample_submission = pd.read_csv(sample_submission_path)\n    \n    # Replace the sample probabilities with the actual predictions\n    for i in range(0, len(predictions)):\n        prediction = predictions[i]\n        for image_id, probs in prediction.items():\n            sample_submission.loc[sample_submission['id'] == image_id, sample_submission.columns[1:]] = probs\n\n    # Save the modified sample submission as the final submission\n    sample_submission.to_csv(output_path, index=False)\n\n# Example usage\n\nclass Dataset_Interpreter_test(Dataset):\n    def __init__(self, data_path, file_names, labels=None):\n        self.data_path = data_path\n        self.file_names = file_names\n        self.labels = labels\n        self.transforms = transforms\n        self.resize = Resize((224,224))\n        self.normalize = Normalize(means, stds)\n        self.trans = transforms.Compose([   \n            transforms.ToTensor()\n        ])\n        \n    def __len__(self):\n        return (len(self.file_names))\n    \n    def __getitem__(self, idx):\n        img_name = f'{self.file_names.iloc[idx]}'\n        full_address = os.path.join(self.data_path, img_name)\n        image = Image.open(full_address)\n        image = self.resize(image)\n        image = self.trans(image)\n        image = self.normalize(image)\n#         if self.transforms is not None:\n#             image = self.transforms(image)\n        \n        return image\n    \nfiles = [f for f in os.listdir(data_dir + '/test') if os.path.isfile(f)]\nfiles = {'id': files}\nfiles = pd.DataFrame(files)\ntest_data = Dataset_Interpreter_valid_test(data_path=data_dir+'/test', file_names=files['id'])    \ntest_loader = DataLoader(test_data, batch_size = batch_size, shuffle = False)\n\ndef test_model(model, data_loader):\n    model.eval() # Set model to eval mode\n    predictions = []\n\n    with torch.no_grad(): # Deactivate gradients for the following code\n        for data_inputs, data_labels in data_loader:\n\n            # Determine prediction of model on dev set\n            data_inputs = data_inputs.to(device)\n            data_labels = data_labels.to(device)\n            preds = model(data_inputs)\n            preds = F.softmax(preds, dim=1)\n            predictions.append(preds)\n        \n    return predictions\n# Assuming you have already defined 'model' and 'train_loader'\nraw_predictions = test_model(model, test_loader)\n\npredictions = []\nfor i in range(0, len(raw_predictions)):\n    prediction = raw_predictions[i]\n    image_id = files.iloc[i]['id']\n    image_id = image_id.split('.jpg')[0]\n    predictions.append({'id': image_id, 'probs': prediction})\n    \n# predictions = {\n#     '000621fb3cbb32d8935728e48679680e': [0.01, 0.02, ...],  # Replace with actual probabilities\n#     # ... add more predictions\n# }\ngenerate_submission(predictions, data_dir + '/sample_submission.csv', 'my_submission.csv')","metadata":{},"execution_count":null,"outputs":[]}]}